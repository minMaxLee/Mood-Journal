{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mood Journal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description \n",
    "As part of my New Year's Resolution of 2020 and in an effort to better remember my life, I began writing a mood journal. Everyday, I would write down a few paragraphs about my day and rank my mood with a number between -3 to +3. It was when I took Introduction to Data Mining that spring semester that I realized I could implement a supervised machine learning model on my journal and observe how the description of my day is related to my mood.\n",
    "\n",
    "### Method \n",
    "Before diving into the details and the code, here is a brief overview of my approach. I used a private Instagram account as my mood journal, attaching a photo and caption for each day. I first needed to extract the text. Once I achieved this, I used the 'bag of words' approach to summarize the textual data. And finally, I used various supervised machine learning classifiers to model and predict my mood. \n",
    "\n",
    "#### Data Extraction\n",
    "I donwloaded a json file with all the relevant data of my Instagram account and I extracted the json file into a Python dictionary. As I had used a range of -3 to +3 in my mood, I decided to make three labels: positive, zero, and negative. It may have been better to ignore zero and have only two labels. But as shown below, the percentage of zeros was quite siginificant at 35%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text     47\n",
      "Label    47\n",
      "dtype: int64\n",
      "shape: (134, 2)\n",
      "As shown, there are 47 zero values which is 0.35074626865671643 of the dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#extract json file into python \n",
    "notebook_path = os.path.abspath(\"MoodJournal.ipynb\")\n",
    "with open(os.path.dirname(notebook_path) + '/../media.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "arr = data['photos']\n",
    "arr.reverse()\n",
    "\n",
    "mood = []\n",
    "time = []\n",
    "text = []\n",
    "data = {'Text': [],\n",
    "        'Label': [] }\n",
    "#extracting features and labels\n",
    "for con in arr:\n",
    "    caption = con['caption']\n",
    "    if len(caption) != 0:\n",
    "        if caption.find('[') != -1 and caption.find(']') != -1 and con['taken_at'] not in time:\n",
    "            rank = caption[caption.index('[') + 1 : caption.index(']')]\n",
    "            description = caption[caption.index(']') + 1 : ]\n",
    "            time.append(con['taken_at'])\n",
    "            # parsing mood label into integer\n",
    "            if '+' in rank:\n",
    "                mood.append(int(rank))\n",
    "            elif '-' in rank: \n",
    "                mood.append(int(rank))\n",
    "            elif rank.isnumeric():\n",
    "                mood.append(int(rank))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # label as positive, zero, negative\n",
    "            rank = int(rank)\n",
    "            if rank < 0:\n",
    "                rank = -1\n",
    "            elif rank > 0:\n",
    "                rank = 1\n",
    "            else:\n",
    "                rank = 0\n",
    "            data['Label'].append(int(rank))\n",
    "            text.append(description)\n",
    "data['Text'] = text\n",
    "df = pd.DataFrame(data)\n",
    "texts = df['Text'].astype(str)\n",
    "y = df['Label']\n",
    "\n",
    "\n",
    "# print(\"number of rows, columns: \",df.shape)\n",
    "pd.set_option('display.max_rows', None)\n",
    "#checking number of zeroes\n",
    "print(df[df['Label'] == 0].count())\n",
    "print(\"shape: \" + str(df.shape))\n",
    "print(\"As shown, there are 47 zero values which is \" + str(47/134) + \" of the dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Vectorizing words\n",
    "Given a list of words, I could use the 'Bag of Words' approach. This is based on Naive Bayes probability, as we assume that each word is independent of each other and that we can draw meaningful conclusion from the frequency of each word. Thus, I can 'vectorize' the words in my mood journal by getting creating a 1d array with the frequency of each word in its respective entry. I used CountVectorizer to do this process.\n",
    "\n",
    "An important process in this to filter the words to reduce redundancy, as otherwise the vector would become too large and specific. One method to reduce redundancy is 'stemming'. This involves trimming down a word to its 'natural root', thereby grouping similar words into one word. Additionally, I used the default 'stop words' which doesn't add any word in the stop words to be in the vector. Common stop words include 'the' and 'a'. Finally, \n",
    "\n",
    "__play around with different settings of preprocessing__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: (134, 1263)\n"
     ]
    }
   ],
   "source": [
    "#using stemming to reduce word counts \n",
    "porter_stemmer = PorterStemmer()\n",
    "def my_preprocessor(text):\n",
    "    text = re.sub(\"\\\\W\", \" \", text)\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", text).lower().split()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "#vectorize the text (bag of words)\n",
    "#using min_df=0.01 as there are 134 documents (rows), so will be accepted if appears more than once\n",
    "#preprocessor = my_preprocessor -> lower accuracy\n",
    "vectorizer = CountVectorizer(stop_words= 'english', min_df=0.01, preprocessor=my_preprocessor)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "#filtering out words\n",
    "# print(vectorizer.get_feature_names())\n",
    "print(\"size: \" + str(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Modelling \n",
    "I chose four different supervised modelling techniques: support vector machine, decision tree, Naive Bayes and linear regression. As the number of words was high, I expected decision tree to suffer from the curse of dimensionality and thus result in lowest accuracy. \n",
    "\n",
    "I calculated each model's accuracy through a cross-validation loop to prevent over-fitting and unbalanced data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62068966 0.62962963 0.65384615 0.38461538 0.61538462]\n",
      "SVC:  0.5808330877296395\n",
      "[0.48275862 0.40740741 0.38461538 0.42307692 0.5       ]\n",
      "tree:  0.42310639552018864\n",
      "[0.44827586 0.48148148 0.5        0.42307692 0.42307692]\n",
      "naive:  0.4551822379408586\n",
      "[0.62068966 0.59259259 0.69230769 0.34615385 0.57692308]\n",
      "regression:  0.5657333726299243\n",
      "('wasn', 0.6771447822713577)\n",
      "('everyth', 0.4893144472959386)\n",
      "('forgot', 0.43602633992652334)\n",
      "('food', 0.3903594171681731)\n",
      "('bad', 0.3742116809889241)\n",
      "('did', -0.45665752715382263)\n",
      "('good', -0.4153777740518988)\n",
      "('algo', -0.41021722698426893)\n",
      "('went', -0.40572422379352713)\n",
      "('thi', -0.34306087368847205)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LinearSVC(class_weight='balanced', dual= False, tol = 1e-2, max_iter= 1e5)\n",
    "print(cross_val_score(model, X, y, cv=5))\n",
    "print(\"SVC: \", cross_val_score(model, X, y, cv=5).mean())\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion = 'entropy')\n",
    "print(cross_val_score(tree, X, y, cv=5))\n",
    "print(\"tree: \", cross_val_score(tree, X, y, cv=5).mean())\n",
    "\n",
    "guassian = GaussianNB()\n",
    "print(cross_val_score(guassian, X.todense(), y, cv=5))\n",
    "print(\"naive: \", cross_val_score(guassian, X.todense(), y, cv=5).mean())\n",
    "\n",
    "regression = LogisticRegression()\n",
    "print(cross_val_score(regression, X, y, cv=5))\n",
    "print(\"regression: \", cross_val_score(regression, X, y, cv = 5).mean())\n",
    "\n",
    "\n",
    "#checking most popular words associated with good and bad mood\n",
    "final_model = regression\n",
    "final_model.fit(X, y)\n",
    "feature_to_coef = {word: coef for word, coef in zip(vectorizer.get_feature_names(), final_model.coef_[0])}\n",
    "for best_positive in sorted(feature_to_coef.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print (best_positive)\n",
    "\n",
    "for best_negative in sorted(feature_to_coef.items(), key=lambda x: x[1])[:5]:\n",
    "    print (best_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusion \n",
    "As predicted, the decision tree algorithm gave the worst average accuracy. Surprisingly, however, was Naive Bayes algorithm results were almost as low as the decision tree's. Perhaps this is because the words are not truly independent of each other, distorting the results. Linear SVM performed the best, at 0.58, as its decision boundaries are linear and thus not affected by the curse of dimensionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
