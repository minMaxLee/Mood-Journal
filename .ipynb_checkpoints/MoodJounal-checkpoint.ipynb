{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mood Journal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description \n",
    "As part of my New Year's Resolution of 2020 and in an effort to better remember my life, I began writing a mood journal. Everyday, I would write down a few paragraphs about my day and rank my mood with a number between -3 to +3. It was when I took Introduction to Data Mining that spring semester that I realized I could implement a supervised machine learning model on my journal and observe how the description of my day is related to my mood.\n",
    "\n",
    "### Method \n",
    "Before diving into the details and the code, here is a brief overview of my approach. I used a private Instagram account as my mood journal, attaching a photo and caption for each day. I first needed to extract the text. Once I achieved this, I used the 'bag of words' approach to summarize the textual data. And finally, I used various supervised machine learning classifiers to model and predict my mood. \n",
    "\n",
    "#### Data Extraction\n",
    "I donwloaded a json file with all the relevant data of my Instagram account and I extracted the json file into a Python dictionary. As I had used a range of -3 to +3 in my mood, I decided to make three labels: positive, zero, and negative. It may have been better to ignore zero and have only two labels. But as shown below, the percentage of zeros was 35%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text     47\n",
      "Label    47\n",
      "dtype: int64\n",
      "(134, 2)\n",
      "As shown, there are 47 zero values which is 0.35074626865671643 of the dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#extract json file into python \n",
    "notebook_path = os.path.abspath(\"MoodJournal.ipynb\")\n",
    "with open(os.path.dirname(notebook_path) + '/../media.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "arr = data['photos']\n",
    "arr.reverse()\n",
    "\n",
    "mood = []\n",
    "time = []\n",
    "text = []\n",
    "data = {'Text': [],\n",
    "        'Label': [] }\n",
    "#extracting features and labels\n",
    "for con in arr:\n",
    "    caption = con['caption']\n",
    "    if len(caption) != 0:\n",
    "        if caption.find('[') != -1 and caption.find(']') != -1 and con['taken_at'] not in time:\n",
    "            rank = caption[caption.index('[') + 1 : caption.index(']')]\n",
    "            description = caption[caption.index(']') + 1 : ]\n",
    "            time.append(con['taken_at'])\n",
    "            # parsing mood label into integer\n",
    "            if '+' in rank:\n",
    "                mood.append(int(rank))\n",
    "            elif '-' in rank: \n",
    "                mood.append(int(rank))\n",
    "            elif rank.isnumeric():\n",
    "                mood.append(int(rank))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # label as positive, zero, negative\n",
    "            rank = int(rank)\n",
    "            if rank < 0:\n",
    "                rank = -1\n",
    "            elif rank > 0:\n",
    "                rank = 1\n",
    "            else:\n",
    "                rank = 0\n",
    "            data['Label'].append(int(rank))\n",
    "            text.append(description)\n",
    "data['Text'] = text\n",
    "df = pd.DataFrame(data)\n",
    "texts = df['Text'].astype(str)\n",
    "y = df['Label']\n",
    "\n",
    "\n",
    "# print(\"number of rows, columns: \",df.shape)\n",
    "pd.set_option('display.max_rows', None)\n",
    "#checking number of zeroes\n",
    "print(df[df['Label'] == 0].count())\n",
    "print(df.shape)\n",
    "print(\"As shown, there are 47 zero values which is \" + str(47/134) + \" of the dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Vectorizing words\n",
    "Given a list of words, I could use the 'Bag of Words' approach. This is based on the Naive Bayes probability, as we assume that each word is independent of each other and that we can draw meaningful conclusion from the frequency of each word. An important process in this to filter the words. I chose to use 'stemming' which involves trimming down a word to its 'natural root', thereby eliminating redudant words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#using stemming to reduce word counts \n",
    "porter_stemmer = PorterStemmer()\n",
    "def my_preprocessor(text):\n",
    "    text = re.sub(\"\\\\W\", \" \", text)\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", text).lower().split()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "#vectorize the text (bag of words)\n",
    "#using min_df=0.01 as there are 134 documents (rows), so will be accepted if appears more than once\n",
    "#preprocessor = my_preprocessor -> lower accuracy\n",
    "vectorizer = CountVectorizer(stop_words= 'english', min_df=0.01, preprocessor=my_preprocessor)\n",
    "# X = vectorizer.fit_transform(texts)\n",
    "\n",
    "#filtering out words\n",
    "# print(vectorizer.get_feature_names())\n",
    "# print(\"size: \" + str(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
